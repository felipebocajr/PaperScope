{
  "generated_at": "2026-02-20T02:07:50.818132Z",
  "model": "GPT-OSS 120B",
  "query": "cat:cs.AI",
  "papers": [
    {
      "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
      "authors": [
        "Yu Xie",
        "Ludwig Winkler",
        "Lixin Sun",
        "Sarah Lewis",
        "Adam E. Foster",
        "José Jiménez Luna",
        "Tim Hempel",
        "Michael Gastegger",
        "Yaoyi Chen",
        "Iryna Zaporozhets",
        "Cecilia Clementi",
        "Christopher M. Bishop",
        "Frank Noé"
      ],
      "published": "2026-02-18",
      "summary": "<article>Rare‑event bottlenecks have kept protein‑folding simulations hostage to weeks of MD, even after diffusion models like BioEmu eliminated slow mixing. The authors close the remaining gap by steering pretrained diffusion samplers toward low‑probability regions and then exactly reweighting the biased ensembles back to equilibrium. Their framework, called enhanced diffusion sampling, adapts classic bias‑and‑reweight techniques—umbrella sampling, metadynamics, and free‑energy tilting—to the generative diffusion setting, yielding three concrete algorithms: UmbrellaDiff, MetaDiff, and ΔG‑Diff. In a test on proteins ranging from 50 to 200 residues, a single tilted ensemble required only about 25 000 diffusion samples (GPU minutes) to recover folding free energies that would need billions of MD steps for the same accuracy. By treating the bias as a controllable drift during the reverse diffusion process, the method produces independent, weighted configurations that avoid kinetic traps and need no hand‑crafted reaction coordinates. This matters because engineers can now obtain unbiased thermodynamic observables—folding stabilities, binding affinities, reaction barriers—on commodity GPUs, turning what was once a multi‑day, high‑performance‑computing task into a routine step in the molecular‑design pipeline.</article>",
      "pdf_url": "https://arxiv.org/pdf/2602.16634v1",
      "entry_id": "http://arxiv.org/abs/2602.16634v1",
      "score": 8.0,
      "score_reasoning": "The paper proposes novel algorithmic extensions to diffusion models for rare-event sampling and free-energy estimation, representing a methodological contribution to AI/ML-driven simulation. While the application domain is molecular dynamics, the core work advances diffusion-based sampling techniques.",
      "innovation": 8.5,
      "impact": 8.0,
      "methodology": 8.5,
      "weighted_score": 8.35,
      "detailed_reasoning": "This paper makes a significant contribution by combining diffusion-model equilibrium samplers with classical enhanced sampling techniques to address both the slow-mixing and rare-event bottlenecks in molecular simulation. The key innovation is the Feynman-Kac Corrector steering framework that enables unbiased biasing of pretrained diffusion models at inference time, instantiated through three concrete algorithms (UmbrellaDiff, MetaDiff, ∆G-Diff). While the individual components are not entirely novel, their integration into a unified framework for diffusion models represents a meaningful advance that challenges the assumption that iid sampling alone solves the rare-event problem. The methodology is rigorous, well-explained with clear mathematical formulation, and demonstrates strong empirical results on protein folding free energies (GPU-minutes to hours vs. GPU-years for unbiased sampling). The impact is substantial for biomolecular simulation, though somewhat limited by dependence on accurate pretrained models and the focus on equilibrium rather than dynamical properties."
    },
    {
      "title": "Fast and Scalable Analytical Diffusion",
      "authors": [
        "Xinyi Shang",
        "Peng Sun",
        "Jingyu Lin",
        "Zhiqiang Shen"
      ],
      "published": "2026-02-18",
      "summary": "<article>When you try to generate high‑fidelity images with analytical diffusion, the biggest obstacle isn’t model quality—it’s the need to scan every training example at each denoising step. Shang et al. discovered that this exhaustive search is unnecessary because the posterior distribution that drives the denoiser collapses from a global manifold to a tight neighborhood as the signal‑to‑noise ratio improves, a behavior they call Posterior Progressive Concentration.\n\nLeveraging this insight, they built GO L DDIFF, a training‑free framework that dynamically selects a “Golden Subset” of the most relevant samples. Early in the diffusion process the method uses a coarse, large candidate pool to capture the broad data structure, then progressively narrows the set to a few precise neighbors as the noise fades. The authors prove that the sparse approximation converges to the exact score and replace the biased weight tricks of prior work with an unbiased streaming softmax applied only to the curated subset.\n\nOn the AFHQ benchmark the approach delivers a 71× speedup while matching or surpassing full‑scan baselines, and it finally scales analytical diffusion to ImageNet‑1K, achieving state‑of‑the‑art MSE and r² scores with dramatically lower compute. For engineers, this means you can now enjoy the interpretability of analytical diffusion without the prohibitive runtime, opening a practical path to transparent, large‑scale generative pipelines.</article>",
      "pdf_url": "https://arxiv.org/pdf/2602.16498v1",
      "entry_id": "http://arxiv.org/abs/2602.16498v1",
      "score": 9.0,
      "score_reasoning": "The paper introduces a novel, theoretically grounded framework (GoldDiff) that fundamentally improves the scalability of analytical diffusion models, representing a clear methodological contribution to generative AI. It also provides theoretical analysis and empirical validation, aligning well with AI/ML research priorities.",
      "innovation": 8.2,
      "impact": 7.8,
      "methodology": 8.5,
      "weighted_score": 8.14,
      "detailed_reasoning": "This paper makes a solid contribution by identifying and formalizing Posterior Progressive Concentration—a genuine insight that full-dataset scanning is unnecessary for analytical diffusion. The dynamic subset selection mechanism (GOLDDIFF) is theoretically grounded with rigorous error bounds and represents a meaningful departure from static k-NN approaches. However, the core innovation is somewhat incremental: it applies well-established retrieval and scheduling ideas to a specific problem rather than introducing fundamentally new concepts. The methodology is rigorous with proper theoretical analysis, comprehensive experiments across multiple scales, and strong ablations. The impact is significant for the analytical diffusion community (71× speedup, first ImageNet-1K scaling) but remains somewhat niche since analytical diffusion itself is not yet a dominant paradigm compared to neural denoisers. The work is well-executed and reproducible, making it a strong contribution to an emerging subfield."
    },
    {
      "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
      "authors": [
        "Vasilis Gkolemis",
        "Loukas Kavouras",
        "Dimitrios Kyriakopoulos",
        "Konstantinos Tsopelas",
        "Dimitrios Rontogiannis",
        "Giuseppe Casalicchio",
        "Theodore Dalamagas",
        "Christos Diou"
      ],
      "published": "2026-02-18",
      "summary": "",
      "pdf_url": "https://arxiv.org/pdf/2602.16503v1",
      "entry_id": "http://arxiv.org/abs/2602.16503v1",
      "score": 9.0,
      "score_reasoning": "The paper introduces a novel interpretable model class (CALMs) and a principled training/distillation pipeline, advancing methodology for balancing accuracy and interpretability. It presents clear algorithmic contributions and empirical validation, aligning well with AI/ML research focus.",
      "innovation": 8.2,
      "impact": 7.5,
      "methodology": 8.8,
      "weighted_score": 8.11,
      "detailed_reasoning": "CALM presents a genuinely novel approach to the interpretability-accuracy tradeoff by introducing conditional feature effects—multiple univariate functions per feature activated in different regions defined by simple threshold conditions. This challenges the strict additivity assumption of GAMs while maintaining interpretability better than GA2Ms' opaque pairwise interactions. The methodology is exceptionally solid, combining distillation, heterogeneity-minimization with theoretical justification (Proposition A.1), and region-aware backfitting with formal convergence guarantees (Proposition 3.1). Empirical results are strong and comprehensive, though impact is somewhat limited by the restriction to tabular data and the acknowledged interpretability degradation as interaction complexity increases."
    }
  ]
}