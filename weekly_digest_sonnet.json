{
  "generated_at": "2026-02-20T02:08:25.505378Z",
  "model": "Claude Sonnet 4.5",
  "query": "cat:cs.AI",
  "papers": [
    {
      "title": "Enhanced Diffusion Sampling: Efficient Rare Event Sampling and Free Energy Calculation with Diffusion Models",
      "authors": [
        "Yu Xie",
        "Ludwig Winkler",
        "Lixin Sun",
        "Sarah Lewis",
        "Adam E. Foster",
        "José Jiménez Luna",
        "Tim Hempel",
        "Michael Gastegger",
        "Yaoyi Chen",
        "Iryna Zaporozhets",
        "Cecilia Clementi",
        "Christopher M. Bishop",
        "Frank Noé"
      ],
      "published": "2026-02-18",
      "summary": "<article>\n\nMolecular simulations have long struggled with two fundamental bottlenecks: trajectories spend too long trapped in metastable states, and rare but important configurations appear exponentially infrequently. Recent diffusion models like BioEmu solved the first problem by generating independent equilibrium samples, bypassing the slow mixing of traditional molecular dynamics. This work solves the second by bringing classical enhanced sampling techniques into the diffusion framework.\n\nThe authors introduce three algorithms that steer pretrained diffusion models to oversample rare regions, then recover unbiased statistics through exact reweighting. UmbrellaDiff adapts umbrella sampling by biasing along reaction coordinates without the kinetic trapping that plagues MD simulations. MetaDiff implements metadynamics in batches, treating each bias update as a well-defined thermodynamic state that feeds directly into multistate reweighting. Perhaps most practically, ∆G-Diff computes free energy differences by tilting the sampling distribution until both end states are adequately represented, then combining ensembles with MBAR.\n\nThe real-world impact is dramatic. Computing protein folding free energies for moderately stable proteins would require sampling roughly one unfolded structure per twenty million equilibrium samples with unbiased diffusion models—potentially a GPU-year of compute. The enhanced methods converge within 100 to 1,000 samples across stability ranges from 1 to 6 kcal/mol, reducing costs to GPU-minutes or hours. The framework demonstrates that when generation is decoupled from dynamics, reaction coordinates need only satisfy statistical overlap rather than dynamical connectivity, fundamentally simplifying enhanced sampling design for biomolecular systems.\n\n</article>",
      "pdf_url": "https://arxiv.org/pdf/2602.16634v1",
      "entry_id": "http://arxiv.org/abs/2602.16634v1",
      "score": 8.0,
      "score_reasoning": "The paper proposes novel algorithmic extensions to diffusion models for rare-event sampling and free-energy estimation, representing a methodological contribution to AI/ML-driven simulation. While the application domain is molecular dynamics, the core work advances diffusion-based sampling techniques.",
      "innovation": 8.5,
      "impact": 8.0,
      "methodology": 8.5,
      "weighted_score": 8.35,
      "detailed_reasoning": "This paper makes a significant contribution by combining diffusion-model equilibrium samplers with classical enhanced sampling techniques to address both the slow-mixing and rare-event bottlenecks in molecular simulation. The key innovation is the Feynman-Kac Corrector steering framework that enables unbiased biasing of pretrained diffusion models at inference time, instantiated through three concrete algorithms (UmbrellaDiff, MetaDiff, ∆G-Diff). While the individual components are not entirely novel, their integration into a unified framework for diffusion models represents a meaningful advance that challenges the assumption that iid sampling alone solves the rare-event problem. The methodology is rigorous, well-explained with clear mathematical formulation, and demonstrates strong empirical results on protein folding free energies (GPU-minutes to hours vs. GPU-years for unbiased sampling). The impact is substantial for biomolecular simulation, though somewhat limited by dependence on accurate pretrained models and the focus on equilibrium rather than dynamical properties."
    },
    {
      "title": "Fast and Scalable Analytical Diffusion",
      "authors": [
        "Xinyi Shang",
        "Peng Sun",
        "Jingyu Lin",
        "Zhiqiang Shen"
      ],
      "published": "2026-02-18",
      "summary": "<article>\n\nAnalytical diffusion models offer mathematical transparency by computing denoising scores as closed-form posterior means over training data, but this clarity comes at a severe cost: every timestep requires scanning the entire dataset, making the approach computationally prohibitive at scale. Researchers have now cracked this bottleneck by recognizing that the full training set is not only unnecessary but sometimes actively harmful to generation quality.\n\nThe key insight is what the authors call \"Posterior Progressive Concentration.\" Early in denoising when noise dominates, the posterior spreads diffusely across many training samples and needs broad coverage to capture the global data manifold. As noise diminishes, however, the posterior collapses sharply onto a tight local neighborhood—meaning most of the dataset contributes negligible information while potentially introducing bias. This observation motivated GOLDDIFF, a training-free framework that dynamically adjusts both the candidate pool size and the aggregation budget across timesteps. Using a coarse-to-fine retrieval strategy with downsampled proxy distances, the method selects a \"Golden Subset\" that shrinks as denoising progresses, maintaining accuracy while slashing computational overhead.\n\nThe results are striking: GOLDDIFF achieves a 71× speedup on AFHQ at 64×64 resolution while matching or exceeding full-dataset baselines in reconstruction fidelity. More significantly, the authors demonstrate the first successful application of analytical diffusion to ImageNet-1K, a milestone previously out of reach. For practitioners, this work transforms analytical diffusion from a theoretical curiosity into a viable tool for interpretable, training-free generation at industrial scales.\n\n</article>",
      "pdf_url": "https://arxiv.org/pdf/2602.16498v1",
      "entry_id": "http://arxiv.org/abs/2602.16498v1",
      "score": 9.0,
      "score_reasoning": "The paper introduces a novel, theoretically grounded framework (GoldDiff) that fundamentally improves the scalability of analytical diffusion models, representing a clear methodological contribution to generative AI. It also provides theoretical analysis and empirical validation, aligning well with AI/ML research priorities.",
      "innovation": 8.2,
      "impact": 7.8,
      "methodology": 8.5,
      "weighted_score": 8.14,
      "detailed_reasoning": "This paper makes a solid contribution by identifying and formalizing Posterior Progressive Concentration—a genuine insight that full-dataset scanning is unnecessary for analytical diffusion. The dynamic subset selection mechanism (GOLDDIFF) is theoretically grounded with rigorous error bounds and represents a meaningful departure from static k-NN approaches. However, the core innovation is somewhat incremental: it applies well-established retrieval and scheduling ideas to a specific problem rather than introducing fundamentally new concepts. The methodology is rigorous with proper theoretical analysis, comprehensive experiments across multiple scales, and strong ablations. The impact is significant for the analytical diffusion community (71× speedup, first ImageNet-1K scaling) but remains somewhat niche since analytical diffusion itself is not yet a dominant paradigm compared to neural denoisers. The work is well-executed and reproducible, making it a strong contribution to an emerging subfield."
    },
    {
      "title": "Interpretability-by-Design with Accurate Locally Additive Models and Conditional Feature Effects",
      "authors": [
        "Vasilis Gkolemis",
        "Loukas Kavouras",
        "Dimitrios Kyriakopoulos",
        "Konstantinos Tsopelas",
        "Dimitrios Rontogiannis",
        "Giuseppe Casalicchio",
        "Theodore Dalamagas",
        "Christos Diou"
      ],
      "published": "2026-02-18",
      "summary": "<article>\n\nMost interpretable machine learning models force a hard choice between accuracy and transparency. Generalized additive models offer clear explanations through independent univariate effects but fail when features interact, while GA²Ms capture interactions through pairwise terms but sacrifice the ability to audit individual feature contributions. Conditionally Additive Local Models (CALMs) resolve this tension by learning multiple univariate shape functions per feature, each active in different regions of the input space where interactions are weak.\n\nThe training process unfolds in three steps. First, a black-box model captures the full complexity of the data. Second, the method partitions the feature space independently for each feature using decision trees that minimize interaction-related heterogeneity, a measure quantifying how much a feature's effect varies with context. Third, region-specific shape functions are fit using a modified backfitting procedure. For example, if age interacts with BMI, CALM might learn separate age curves for patients above and below BMI threshold 30, with each curve remaining interpretable as a simple univariate plot.\n\nAcross 25 real-world tabular datasets, CALMs outperform standard GAMs in 23 of 25 cases and match or exceed GA²M accuracy in many scenarios while using remarkably few interactions—averaging just 6.2 for classification tasks compared to 97.7 for competing methods. The approach preserves key interpretability properties that GA²Ms lose: transparent feature attribution, clear regional sensitivity analysis, and straightforward verification of global properties like monotonicity. For practitioners building models in regulated domains, CALMs offer a practical middle ground that doesn't compromise explainability for predictive power.\n\n</article>",
      "pdf_url": "https://arxiv.org/pdf/2602.16503v1",
      "entry_id": "http://arxiv.org/abs/2602.16503v1",
      "score": 9.0,
      "score_reasoning": "The paper introduces a novel interpretable model class (CALMs) and a principled training/distillation pipeline, advancing methodology for balancing accuracy and interpretability. It presents clear algorithmic contributions and empirical validation, aligning well with AI/ML research focus.",
      "innovation": 8.2,
      "impact": 7.5,
      "methodology": 8.8,
      "weighted_score": 8.11,
      "detailed_reasoning": "CALM presents a genuinely novel approach to the interpretability-accuracy tradeoff by introducing conditional feature effects—multiple univariate functions per feature activated in different regions defined by simple threshold conditions. This challenges the strict additivity assumption of GAMs while maintaining interpretability better than GA2Ms' opaque pairwise interactions. The methodology is exceptionally solid, combining distillation, heterogeneity-minimization with theoretical justification (Proposition A.1), and region-aware backfitting with formal convergence guarantees (Proposition 3.1). Empirical results are strong and comprehensive, though impact is somewhat limited by the restriction to tabular data and the acknowledged interpretability degradation as interaction complexity increases."
    }
  ]
}